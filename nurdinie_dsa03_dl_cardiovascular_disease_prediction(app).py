# -*- coding: utf-8 -*-
"""NURDINIE_DSA03_DL_Cardiovascular_Disease_Prediction(App).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fCwnOVOe74ssF4dY7LXYRxQJTHZtjKlk

#STEP 1) DATA LOADING
"""

import os
import pickle
import numpy as np
import pandas as pd
import seaborn as sns
import scipy.stats as ss
import matplotlib.pyplot as plt

from sklearn.svm import SVC
from sklearn import pipeline
from sklearn.pipeline import Pipeline
from sklearn.impute import KNNImputer
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import MinMaxScaler, StandardScaler

def cramers_corrected_stat(confusion_matrix):
    """ calculate Cramers V statistic for categorial-categorial association.
        uses correction from Bergsma and Wicher,
        Journal of the Korean Statistical Society 42 (2013): 323-328
    """
    chi2 = ss.chi2_contingency(confusion_matrix)[0]
    n = confusion_matrix.sum()
    phi2 = chi2/n
    r,k = confusion_matrix.shape
    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))
    rcorr = r - ((r-1)**2)/(n-1)
    kcorr = k - ((k-1)**2)/(n-1)
    return np.sqrt(phi2corr / min( (kcorr-1), (rcorr-1)))

def plot_con_graph(cat,df):
    '''
    This function is meant to plot continuous data using seaborn
    distplot function

    Parameters
    ----------
    con : List
        con(contain the name of the continuous columns.
    df : DATAFRAME
        df contains the data.

    Returns
    -------
    None.

    '''
    for i in cat:
        plt.figure()
        sns.distplot(df[i])
        plt.show()

def plot_con_graph(con,df):
    '''
    This function is meant to plot continuous data using seaborn
    distplot function

    Parameters
    ----------
    con : List
        con(contain the name of the continuous columns.
    df : DATAFRAME
        df contains the data.

    Returns
    -------
    None.

    '''
    for i in con:
        plt.figure()
        sns.distplot(df[i])
        plt.show()

CSV_PATH = os.path.join(os.getcwd(), 'cardio_train.csv') # current working directory

column_names = ['ID', 'AGE', 'GENDER', 'HEIGHT', 'WEIGHT', 'AP_HI', 'AP_LO', 'CHOLESTEROL', 'GLUC', 'SMOKE', 'ALCO', 'ACTIVE', 'CARDIO']
df = pd.read_csv(CSV_PATH, delimiter = ';', names = column_names, header = 0)
df.head()

"""#STEP 2) DATA VISUALIZATION / INSPECTION"""

df.info()

"""**Check data distribution**"""

df.describe().T

"""**Check NaNs**"""

df.isna().sum()

"""We can observe that there are 6 NaNs identified from HEIGHT(2), WEIGHT(1), CHOLESTEROL(2), and ALCO(1).

**Check duplicates**
"""

df.duplicated().sum()

df[df.duplicated()]

"""We discover that there are 3 duplicates which are ID 61, 119, and 178.

**Chcek outliers**
"""

df.boxplot()

"""As shown above, there are outliers detected."""

cat = ['GENDER', 'CHOLESTEROL', 'GLUC', 'SMOKE', 'ALCO', 'ACTIVE', 'CARDIO'] # CATEGORICAL FEATURES
con = ['AGE', 'HEIGHT', 'WEIGHT', 'AP_HI', 'AP_LO'] # CONTINOUS FEATURES

"""**For categorical data:**"""

plot_con_graph(cat, df)

# another method to display categorical data only (OPTIONAL)
df.groupby(['GENDER', 'CARDIO']).agg({'CARDIO':'count'})

# EARLY HYPOTHESIS
# 1: MALE 2: FEMALE
df.groupby(['GENDER', 'CARDIO']).agg({'CARDIO':'count'}).plot(kind='bar')
df.groupby(['GENDER', 'CARDIO']).agg({'CARDIO':'count'}).plot(kind='bar')
df.groupby(['GENDER', 'SMOKE']).agg({'SMOKE':'count'}).plot(kind='bar')

"""**For continous data:**"""

plot_con_graph(con, df)

"""Observation from above:

ap_hi: Systolic 80<Sys<370 Reasonable range

ap_lo: Diastolic 60<Sys<370 Reasonable range

ap_hi has -ve & ooutliers

ap_lo has -ve & ooutliers

**Check data distribution**
"""

df.describe().T

df.duplicated().sum()

"""#STEP 3) DATA CLEANING

**Remove ID column**
"""

df = df.drop(labels=['ID'], axis=1)

"""**1) Outliers (-ve & very huge number)**

*Remove outliers*
"""

(df['AP_HI']>370).sum() # 39 smaples more than 370 Systolic

df['AP_HI'][df['AP_LO']>370]

"""*Clipping*"""

df['AP_HI'] = df['AP_HI'].clip(80,370)
df['AP_LO'] = df['AP_LO'].clip(60,370)

df.describe().T

"""*For continous data:*"""

plot_con_graph(con, df)

"""**2) NaNs**

*KNN Imputation*
"""

knn_imputer = KNNImputer()
df_knn = knn_imputer.fit_transform(df)

df_knn = pd.DataFrame(df_knn,index=None) #to convert into DataFrame format
df_knn.columns = df.columns
df_knn.isna().sum()

df_knn['CHOLESTEROL'] = np.floor(df_knn['CHOLESTEROL']).astype('int')
df_knn['ALCO'] = np.floor(df_knn['ALCO']).astype('int')

df_knn.describe().T

df_knn.isna().sum()

# Method 4) Iterative Imputation
#from sklearn.experimental import enable_iterative_imputer
#from sklearn.impute import IterativeImputer

#ii = IterativeImputer()
#ii_df = ii.fit_transform(df)
#ii_df = pd.DataFrame(ii_df, index=None)
#ii_df.columns = df.columns

#ii_df.describe().T

#ii_df.isna().sum() # to double check if NaN still exist

"""**3) Duplicated**"""

df = df.drop_duplicates(inplace=True)

"""#STEP 4) FEATURE SELECTION

**cat vs cat**

cramer's v
"""

for i in cat:
  print(i)
  confusion_matrix = pd.crosstab(df_knn[i], df_knn['CARDIO']).to_numpy()
  print(cramers_corrected_stat(confusion_matrix))

"""Only cholestrol has correlation with CVD

**con vs cat**
"""

for i in con:
  print(i)
  lr = LogisticRegression()
  lr.fit(np.expand_dims(df_knn[i], axis=1), df_knn['CARDIO']) #X:continous
  print(lr.score(np.expand_dims(df_knn[i], axis=1), df_knn['CARDIO'])) #X:continous

"""Cholestrol + Ap_hi and Ap_lo has correslation with cardio

#STEP 5) PREPROCESSING
"""

X = df_knn.loc[:,{'CHOLESTEROL','AP_HI','AP_LO'}]
y = df_knn['CARDIO']

X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                    test_size=0.3,
                                                    random_state=123)

"""# MODEL DEVELOPMENT --> PIPELINES

*Logistic Regression*
"""

pipeline_mms_lr = Pipeline([
                           ('Min_Max_Scaler',MinMaxScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
]) # Pipeline([STEPS])

pipeline_ss_lr = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Logistic_Classifier', LogisticRegression())
]) # Pipeline([STEPS])

"""*Decision Tree*"""

pipeline_mms_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Tree_Classifier', DecisionTreeClassifier())
]) # Pipeline([STEPS])

pipeline_ss_dt = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Tree_Classifier', DecisionTreeClassifier())
]) # Pipeline([STEPS])

"""*Random Forest*"""

pipeline_mms_rf = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Forest_Classifier', RandomForestClassifier())
]) # Pipeline([STEPS])

pipeline_ss_rf = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('Forest_Classifier', RandomForestClassifier())
]) # Pipeline([STEPS])

"""*Gradient Boost*"""

pipeline_mms_gb = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('GBoost_Classifier', GradientBoostingClassifier())
]) # Pipeline([STEPS])

pipeline_ss_gb = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('GBoost_Classifier', SVC())
]) # Pipeline([STEPS])

"""*SVC*"""

pipeline_mms_svc = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('SVC_Classifier', SVC())
]) # Pipeline([STEPS])

pipeline_ss_svc = Pipeline([
                           ('Standard_Scaler',StandardScaler()), 
                           ('SVC_Classifier', SVC())
]) # Pipeline([STEPS])

"""**To create a list to store all the pipelines**"""

pipelines = [pipeline_mms_lr, pipeline_ss_lr,
             pipeline_mms_dt, pipeline_ss_dt,
             pipeline_mms_rf, pipeline_ss_rf,
             pipeline_mms_gb, pipeline_ss_gb,
             pipeline_mms_svc, pipeline_ss_svc]

for pipe in pipelines:
  pipe.fit(X_train, y_train)

for i, pipe in enumerate(pipelines):
  print(pipe.score(X_test, y_test))

"""**Best Accuracy**"""

best_accuracy = 0

for i, pipe in enumerate(pipelines):
  #print(pipe.score(X_test, y_test))
  if pipe.score(X_test, y_test) > best_accuracy:
    best_accuracy = pipe.score(X_test, y_test)
    best_pipeline = pipe

print('The best scaler and classifier for cardio train dataset is {} with accuracy of {}'
.format(best_pipeline.steps,best_accuracy))

"""**GridSearchCV --> save the best estimator**"""

# Grid Search CV
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
pipeline_mms_gs = Pipeline([
                           ('Min_Max_Scaler',MinMaxScaler()), 
                           ('GBoost_Classifier', GradientBoostingClassifier())
]) # Pipeline([STEPS])

grid_params = [{"GBoost_Classifier__max_depth": np.arange(2, 10, 1),
                "GBoost_Classifier__max_leaf_nodes": [None, 3, 5],
                "GBoost_Classifier__min_samples_leaf": [1,3,5]
                }]

grid_search = GridSearchCV(estimator=pipeline_mms_gs,
                           param_grid = grid_params,
                           verbose = 1,
                           cv=5,
                           n_jobs=-1)

grid = grid_search.fit(X_train,y_train)
grid.score(X_test, y_test)
print(grid.best_params_)

"""**Model Savings**"""

import pickle
import os

BEST_ESTIMATOR_SAVE_PATH=os.path.join(os.getcwd(),'best_estimator.pkl')

with open(BEST_ESTIMATOR_SAVE_PATH, 'wb') as file:
    pickle.dump(grid.best_estimator_,file)